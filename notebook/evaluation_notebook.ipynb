{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OSEDoyDfLa92"
      },
      "outputs": [],
      "source": [
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def monte_carlo_eval(prompt):\n",
        "    # Simulating different types of responses\n",
        "    response_types = ['highly relevant', 'somewhat relevant', 'irrelevant']\n",
        "    scores = {'highly relevant': 3, 'somewhat relevant': 2, 'irrelevant': 1}\n",
        "\n",
        "    # Perform multiple random trials\n",
        "    trials = 100\n",
        "    total_score = 0\n",
        "    for _ in range(trials):\n",
        "        response = random.choice(response_types)\n",
        "        total_score += scores[response]\n",
        "\n",
        "    # Average score represents the evaluation\n",
        "    return total_score / trials\n",
        "\n",
        "def elo_eval(prompt, base_rating=1500):\n",
        "    # Simulate the outcome of the prompt against standard criteria\n",
        "    # Here, we randomly decide if the prompt 'wins', 'loses', or 'draws'\n",
        "    outcomes = ['win', 'loss', 'draw']\n",
        "    outcome = random.choice(outcomes)\n",
        "\n",
        "    # Elo rating formula parameters\n",
        "    K = 30  # Maximum change in rating\n",
        "    R_base = 10 ** (base_rating / 400)\n",
        "    R_opponent = 10 ** (1600 / 400)  # Assuming a fixed opponent rating\n",
        "    expected_score = R_base / (R_base + R_opponent)\n",
        "\n",
        "    # Calculate the new rating based on the outcome\n",
        "    actual_score = {'win': 1, 'loss': 0, 'draw': 0.5}[outcome]\n",
        "    new_rating = base_rating + K * (actual_score - expected_score)\n",
        "\n",
        "    return new_rating\n",
        "def elo_ratings_func(prompts, elo_ratings, K=30, opponent_rating=1600):\n",
        "    \"\"\"\n",
        "    Update Elo ratings for a list of prompts based on simulated outcomes.\n",
        "\n",
        "    Parameters:\n",
        "    prompts (list): List of prompts to be evaluated.\n",
        "    elo_ratings (dict): Current Elo ratings for each prompt.\n",
        "    K (int): Maximum change in rating.\n",
        "    opponent_rating (int): Fixed rating of the opponent for simulation.\n",
        "\n",
        "    Returns:\n",
        "    dict: Updated Elo ratings.\n",
        "    \"\"\"\n",
        "\n",
        "    for prompt in prompts:\n",
        "        # Simulate an outcome against the standard criteria or another prompt\n",
        "        outcome = random.choice(['win', 'loss', 'draw'])\n",
        "\n",
        "        # Calculate the new rating based on the outcome\n",
        "        actual_score = {'win': 1, 'loss': 0, 'draw': 0.5}[outcome]\n",
        "        R_base = 10 ** (elo_ratings[prompt] / 400)\n",
        "        R_opponent = 10 ** (opponent_rating / 400)\n",
        "        expected_score = R_base / (R_base + R_opponent)\n",
        "        elo_ratings[prompt] += K * (actual_score - expected_score)\n",
        "\n",
        "    return elo_ratings\n",
        "\n",
        "# Example usage\n",
        "prompts = [\"What is the historical origin of machine learning?\",\n",
        "                \"What are the core components of machine learning?\",\n",
        "                \"How has machine learning evolved over time?\",\n",
        "                \"What is the primary goal of machine learning?\",\n",
        "                \"What are some potential applications of machine learning?\"\n",
        "                ]\n",
        "elo_ratings = {prompt: 1500 for prompt in prompts}  # Initial ratings\n",
        "\n",
        "# Conduct multiple rounds of evaluation\n",
        "for _ in range(10):  # Number of rounds\n",
        "    elo_ratings = elo_ratings_func(prompts, elo_ratings)\n",
        "\n",
        "# Sort prompts by their final Elo ratings\n",
        "sorted_prompts = sorted(prompts, key=lambda x: elo_ratings[x], reverse=True)\n",
        "\n",
        "# Print the ranked prompts\n",
        "for prompt in sorted_prompts:\n",
        "    print(f\"{prompt}: {elo_ratings[prompt]}\")"
      ],
      "metadata": {
        "id": "Kbg1_RPhMGNg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_prompt(main_prompt, test_cases):\n",
        "    evaluations = {}\n",
        "\n",
        "    # Evaluate the main prompt using Monte Carlo and Elo methods\n",
        "    evaluations['main_prompt'] = {\n",
        "        'Monte Carlo Evaluation': monte_carlo_eval(main_prompt),\n",
        "        'Elo Rating Evaluation': elo_eval(main_prompt)\n",
        "    }\n",
        "\n",
        "    # Evaluate each test case\n",
        "    for idx, test_case in enumerate(test_cases):\n",
        "        evaluations[f'test_case_{idx+1}'] = {\n",
        "            'Monte Carlo Evaluation': monte_carlo_eval(test_case),\n",
        "            'Elo Rating Evaluation': elo_eval(test_case)\n",
        "        }\n",
        "\n",
        "    return evaluations"
      ],
      "metadata": {
        "id": "pphqQSmHMedg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main_prompt = \"why we use Machine learning?\"\n",
        "test_cases = [\"What is the historical origin of machine learning?\",\n",
        "                \"What are the core components of machine learning?\",\n",
        "                \"How has machine learning evolved over time?\",\n",
        "                \"What is the primary goal of machine learning?\",\n",
        "                \"What are some potential applications of machine learning?\"\n",
        "             ]\n",
        "result = evaluate_prompt(main_prompt, test_cases)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "VecnE0UkMt7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **RAGAS Evaluation**"
      ],
      "metadata": {
        "id": "WSZB6SvIPYBn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Weaviate\n",
        "import weaviate\n",
        "from weaviate.embedded import EmbeddedOptions\n",
        "from dotenv import load_dotenv,find_dotenv\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Weaviate\n",
        "import weaviate\n",
        "from weaviate.embedded import EmbeddedOptions\n",
        "from dotenv import load_dotenv,find_dotenv\n",
        "#\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "from langchain.schema.output_parser import StrOutputParser"
      ],
      "metadata": {
        "id": "VTQnSGIFPbVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data loader\n",
        "def data_loader(file_path= 'prompts/context.txt'):\n",
        "    loader = TextLoader(file_path)\n",
        "    documents = loader.load()\n",
        "\n",
        "    # Chunk the data\n",
        "    text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "    chunks = text_splitter.split_documents(documents)\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "hyYl56oEPn3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_retriever(chunks):\n",
        "\n",
        "  # Load OpenAI API key from .env file\n",
        "  load_dotenv(find_dotenv())\n",
        "\n",
        "  # Setup vector database\n",
        "  client = weaviate.Client(\n",
        "    embedded_options = EmbeddedOptions()\n",
        "  )\n",
        "\n",
        "  # Populate vector database\n",
        "  vectorstore = Weaviate.from_documents(\n",
        "      client = client,\n",
        "      documents = chunks,\n",
        "      embedding = OpenAIEmbeddings(),\n",
        "      by_text = False\n",
        "  )\n",
        "\n",
        "  # Define vectorstore as retriever to enable semantic search\n",
        "  retriever = vectorstore.as_retriever()\n",
        "  return retriever"
      ],
      "metadata": {
        "id": "IqdH2LqwPpcz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunks"
      ],
      "metadata": {
        "id": "FihOym7VPvfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunks =  data_loader()\n",
        "retriever = create_retriever(chunks)"
      ],
      "metadata": {
        "id": "21KVRq2cPzX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define LLM\n",
        "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "\n",
        "# Define prompt template\n",
        "template = \"\"\"You are an assistant for question-answering tasks.\n",
        "Use the following pieces of retrieved context to answer the question.\n",
        "If you don't know the answer, just say that you don't know.\n",
        "Use two sentences maximum and keep the answer concise.\n",
        "Question: {question}\n",
        "Context: {context}\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "# Setup RAG pipeline\n",
        "rag_chain = (\n",
        "    {\"context\": retriever,  \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ],
      "metadata": {
        "id": "ZFl03R_8P8lL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "questions = [\"What is the historical origin of machine learning?\",\n",
        "             \"What are the core components of machine learning?\",\n",
        "             \"How has machine learning evolved over time?\",\n",
        "            ]\n",
        "ground_truths = [[\"Machine learning emerged as a subfield of artificial intelligence in the mid-20th century.\"],\n",
        "                [\"he core components include data preprocessing, feature extraction, model selection, training, and evaluation.\"],\n",
        "                [\"Machine learning has evolved due to advances in computing power, algorithms, and data availability, ranging from traditional statistical methods to sophisticated deep learning algorithms.\"]]\n",
        "answers = []\n",
        "contexts = []\n",
        "\n",
        "# Inference\n",
        "for query in questions:\n",
        "\n",
        "  answers.append(rag_chain.invoke(query))\n",
        "  contexts.append([docs.page_content for docs in retriever.get_relevant_documents(query)])\n",
        "\n",
        "# To dict\n",
        "data = {\n",
        "    \"question\": questions, # list\n",
        "    \"answer\": answers, # list\n",
        "    \"contexts\": contexts, # list list\n",
        "    \"ground_truths\": ground_truths # list Lists\n",
        "}\n",
        "\n",
        "# Convert dict to dataset\n",
        "dataset = Dataset.from_dict(data)"
      ],
      "metadata": {
        "id": "aiKt6VubQESX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ragas import evaluate\n",
        "from ragas.metrics import (\n",
        "    faithfulness,\n",
        "    answer_relevancy,\n",
        "    context_recall,\n",
        "    context_precision,\n",
        ")\n",
        "\n",
        "result = evaluate(\n",
        "    dataset = dataset,\n",
        "    metrics=[\n",
        "        context_precision,\n",
        "        context_recall,\n",
        "        faithfulness,\n",
        "        answer_relevancy,\n",
        "    ],\n",
        ")\n",
        "\n",
        "df = result.to_pandas()"
      ],
      "metadata": {
        "id": "PHan_cHXQU-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A1meSIDWPRcF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}